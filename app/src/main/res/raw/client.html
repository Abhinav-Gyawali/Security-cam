<!DOCTYPE html>
<html>
<head>
    <title>Security Camera Stream</title>
    <style>
        #videoElement {
            width: 100%;
            max-width: 800px;
            height: auto;
        }
    </style>
</head>
<body>
    <video id="videoElement" autoplay muted></video>
    <script>
        const FRAME_TYPE_VIDEO = 0x01;
        const FRAME_TYPE_AUDIO = 0x02;
        const FRAME_TYPE_VIDEO_CONFIG = 0x03;
        const FRAME_TYPE_AUDIO_CONFIG = 0x04;

        let mediaSource;
        let sourceBuffer;
        let queue = [];
        let videoConfig = null;
        let audioConfig = null;

        function init() {
            mediaSource = new MediaSource();
            document.getElementById('videoElement').src = URL.createObjectURL(mediaSource);

            mediaSource.addEventListener('sourceopen', () => {
                // We'll create source buffer once we receive config
                connectWebSocket();
            });
        }

        function connectWebSocket() {
            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            const ws = new WebSocket(`${protocol}//${window.location.host}/stream`);

            ws.binaryType = 'arraybuffer';

            ws.onmessage = (event) => {
                const data = new Uint8Array(event.data);
                const frameType = data[0];
                const nalType = data[1];
                const timestamp = new DataView(event.data).getInt32(2, true);
                const frameData = data.slice(6);

                switch (frameType) {
                    case FRAME_TYPE_VIDEO_CONFIG:
                        videoConfig = frameData;
                        initializeSourceBuffer();
                        break;
                    case FRAME_TYPE_AUDIO_CONFIG:
                        audioConfig = frameData;
                        initializeSourceBuffer();
                        break;
                    case FRAME_TYPE_VIDEO:
                        if (sourceBuffer && !sourceBuffer.updating) {
                            appendVideoFrame(frameData, timestamp);
                        } else {
                            queue.push({data: frameData, timestamp: timestamp, type: 'video'});
                        }
                        break;
                    case FRAME_TYPE_AUDIO:
                        if (sourceBuffer && !sourceBuffer.updating) {
                            appendAudioFrame(frameData, timestamp);
                        } else {
                            queue.push({data: frameData, timestamp: timestamp, type: 'audio'});
                        }
                        break;
                }
            };

            ws.onclose = () => {
                console.log('WebSocket connection closed');
                // Attempt to reconnect after a delay
                setTimeout(connectWebSocket, 5000);
            };

            ws.onerror = (error) => {
                console.error('WebSocket error:', error);
            };
        }

        function initializeSourceBuffer() {
            if (!mediaSource || mediaSource.readyState !== 'open') return;
            if (!videoConfig || !audioConfig) return; // Wait for both configs

            try {
                if (!sourceBuffer) {
                    sourceBuffer = mediaSource.addSourceBuffer('video/mp4; codecs="avc1.640028,mp4a.40.2"');
                    sourceBuffer.addEventListener('updateend', () => {
                        if (queue.length > 0 && !sourceBuffer.updating) {
                            const frame = queue.shift();
                            if (frame.type === 'video') {
                                appendVideoFrame(frame.data, frame.timestamp);
                            } else {
                                appendAudioFrame(frame.data, frame.timestamp);
                            }
                        }
                    });
                }
            } catch (e) {
                console.error('Error creating SourceBuffer:', e);
            }
        }

        function appendVideoFrame(frameData, timestamp) {
            try {
                // Create MP4 fragment with proper NAL unit framing
                const fragment = createMP4Fragment(frameData, timestamp, true);
                sourceBuffer.appendBuffer(fragment);
            } catch (e) {
                console.error('Error appending video frame:', e);
            }
        }

        function appendAudioFrame(frameData, timestamp) {
            try {
                // Create MP4 fragment for audio
                const fragment = createMP4Fragment(frameData, timestamp, false);
                sourceBuffer.appendBuffer(fragment);
            } catch (e) {
                console.error('Error appending audio frame:', e);
            }
        }

        function createMP4Fragment(frameData, timestamp, isVideo) {
            // This is a simplified version - in reality you'd need proper MP4 box creation
            // You'll need to implement proper MP4 fragmenting based on your specific needs
            return frameData; // Placeholder - implement proper MP4 fragmentation
        }

        window.addEventListener('load', init);
    </script>
</body>
</html>